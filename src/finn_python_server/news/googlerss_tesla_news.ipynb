{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40af022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 패키지 설치: pip install feedparser pandas\n",
    "import feedparser\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71905025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_google_rss_url(query, start_date, end_date):\n",
    "    base_url = \"https://news.google.com/rss/search?\"\n",
    "    q = f\"q={query}+after:{start_date}+before:{end_date}\"\n",
    "    params = \"&hl=en-US&gl=US&ceid=US:en\"\n",
    "    return base_url + q + params\n",
    "\n",
    "def fetch_news_rss_day(query, day: datetime):\n",
    "    start_date = day.strftime(\"%Y-%m-%d\")\n",
    "    end_date = (day + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    url = generate_google_rss_url(query, start_date, end_date)\n",
    "    feed = feedparser.parse(url)\n",
    "\n",
    "    items = []\n",
    "    for entry in feed.entries:\n",
    "        try:\n",
    "            pub_date = datetime(*entry.published_parsed[:6])\n",
    "        except Exception:\n",
    "            continue\n",
    "        items.append({\n",
    "            \"date\": pub_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"title\": entry.title,\n",
    "            \"link\": entry.link,\n",
    "            \"source\": entry.source.title if \"source\" in entry else \"Unknown\"\n",
    "        })\n",
    "\n",
    "    return items\n",
    "\n",
    "def get_news_data(start_day):\n",
    "    # ✅ 수집 범위: 2025-01-01 ~ 오늘까지\n",
    "    end_day = datetime.now()\n",
    "\n",
    "    all_news = []\n",
    "    current_day = start_day\n",
    "\n",
    "    while current_day <= end_day:\n",
    "        daily_news = fetch_news_rss_day(\"Tesla\", current_day)\n",
    "        all_news.extend(daily_news)\n",
    "        print(f\"✅ {current_day.strftime('%Y-%m-%d')} - {len(daily_news)} items\")\n",
    "        current_day += timedelta(days=1)\n",
    "    return all_news\n",
    "\n",
    "def remove_duplicate_titles_by_prefix(df, prefix_length=50):\n",
    "    seen = set()\n",
    "    keep_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        prefix = row[\"title\"][:prefix_length].strip().lower()\n",
    "        if prefix not in seen:\n",
    "            seen.add(prefix)\n",
    "            keep_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(keep_rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125f780c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2025-01-01 - 100 items\n",
      "✅ 2025-01-02 - 100 items\n",
      "✅ 2025-01-03 - 80 items\n",
      "✅ 2025-01-04 - 21 items\n",
      "✅ 2025-01-05 - 31 items\n",
      "✅ 2025-01-06 - 78 items\n",
      "✅ 2025-01-07 - 87 items\n",
      "✅ 2025-01-08 - 73 items\n",
      "✅ 2025-01-09 - 100 items\n",
      "✅ 2025-01-10 - 100 items\n",
      "✅ 2025-01-11 - 37 items\n",
      "✅ 2025-01-12 - 57 items\n",
      "✅ 2025-01-13 - 85 items\n",
      "✅ 2025-01-14 - 73 items\n",
      "✅ 2025-01-15 - 63 items\n",
      "✅ 2025-01-16 - 57 items\n",
      "✅ 2025-01-17 - 41 items\n",
      "✅ 2025-01-18 - 19 items\n",
      "✅ 2025-01-19 - 30 items\n",
      "✅ 2025-01-20 - 52 items\n",
      "✅ 2025-01-21 - 60 items\n",
      "✅ 2025-01-22 - 73 items\n",
      "✅ 2025-01-23 - 100 items\n",
      "✅ 2025-01-24 - 73 items\n",
      "✅ 2025-01-25 - 24 items\n",
      "✅ 2025-01-26 - 51 items\n",
      "✅ 2025-01-27 - 78 items\n",
      "✅ 2025-01-28 - 100 items\n",
      "✅ 2025-01-29 - 100 items\n",
      "✅ 2025-01-30 - 100 items\n",
      "✅ 2025-01-31 - 47 items\n",
      "✅ 2025-02-01 - 18 items\n",
      "✅ 2025-02-02 - 63 items\n",
      "✅ 2025-02-03 - 99 items\n",
      "✅ 2025-02-04 - 96 items\n",
      "✅ 2025-02-05 - 35 items\n",
      "✅ 2025-02-06 - 53 items\n",
      "✅ 2025-02-07 - 32 items\n",
      "✅ 2025-02-08 - 16 items\n",
      "✅ 2025-02-09 - 28 items\n",
      "✅ 2025-02-10 - 44 items\n",
      "✅ 2025-02-11 - 49 items\n",
      "✅ 2025-02-12 - 59 items\n",
      "✅ 2025-02-13 - 51 items\n",
      "✅ 2025-02-14 - 29 items\n",
      "✅ 2025-02-15 - 18 items\n",
      "✅ 2025-02-16 - 21 items\n",
      "✅ 2025-02-17 - 30 items\n",
      "✅ 2025-02-18 - 36 items\n",
      "✅ 2025-02-19 - 34 items\n",
      "✅ 2025-02-20 - 55 items\n",
      "✅ 2025-02-21 - 43 items\n",
      "✅ 2025-02-22 - 19 items\n",
      "✅ 2025-02-23 - 41 items\n",
      "✅ 2025-02-24 - 77 items\n",
      "✅ 2025-02-25 - 79 items\n",
      "✅ 2025-02-26 - 60 items\n",
      "✅ 2025-02-27 - 37 items\n",
      "✅ 2025-02-28 - 25 items\n",
      "✅ 2025-03-01 - 21 items\n",
      "✅ 2025-03-02 - 44 items\n",
      "✅ 2025-03-03 - 66 items\n",
      "✅ 2025-03-04 - 86 items\n",
      "✅ 2025-03-05 - 89 items\n",
      "✅ 2025-03-06 - 71 items\n",
      "✅ 2025-03-07 - 45 items\n",
      "✅ 2025-03-08 - 28 items\n",
      "✅ 2025-03-09 - 100 items\n",
      "✅ 2025-03-10 - 100 items\n",
      "✅ 2025-03-11 - 100 items\n",
      "✅ 2025-03-12 - 100 items\n",
      "✅ 2025-03-13 - 100 items\n",
      "✅ 2025-03-14 - 100 items\n",
      "✅ 2025-03-15 - 100 items\n",
      "✅ 2025-03-16 - 100 items\n",
      "✅ 2025-03-17 - 100 items\n",
      "✅ 2025-03-18 - 100 items\n",
      "✅ 2025-03-19 - 100 items\n",
      "✅ 2025-03-20 - 100 items\n",
      "✅ 2025-03-21 - 100 items\n",
      "✅ 2025-03-22 - 100 items\n",
      "✅ 2025-03-23 - 100 items\n",
      "✅ 2025-03-24 - 100 items\n",
      "✅ 2025-03-25 - 100 items\n",
      "✅ 2025-03-26 - 100 items\n",
      "✅ 2025-03-27 - 100 items\n",
      "✅ 2025-03-28 - 100 items\n",
      "✅ 2025-03-29 - 100 items\n",
      "✅ 2025-03-30 - 100 items\n",
      "✅ 2025-03-31 - 100 items\n",
      "✅ 2025-04-01 - 100 items\n",
      "✅ 2025-04-02 - 100 items\n",
      "✅ 2025-04-03 - 100 items\n",
      "✅ 2025-04-04 - 100 items\n",
      "✅ 2025-04-05 - 97 items\n",
      "✅ 2025-04-06 - 100 items\n",
      "✅ 2025-04-07 - 100 items\n",
      "✅ 2025-04-08 - 100 items\n",
      "✅ 2025-04-09 - 100 items\n",
      "✅ 2025-04-10 - 100 items\n",
      "✅ 2025-04-11 - 100 items\n",
      "✅ 2025-04-12 - 100 items\n",
      "✅ 2025-04-13 - 100 items\n",
      "✅ 2025-04-14 - 100 items\n",
      "✅ 2025-04-15 - 100 items\n",
      "✅ 2025-04-16 - 100 items\n",
      "✅ 2025-04-17 - 100 items\n",
      "✅ 2025-04-18 - 100 items\n",
      "✅ 2025-04-19 - 100 items\n",
      "✅ 2025-04-20 - 100 items\n",
      "✅ 2025-04-21 - 100 items\n",
      "✅ 2025-04-22 - 100 items\n",
      "✅ 2025-04-23 - 100 items\n",
      "✅ 2025-04-24 - 100 items\n",
      "✅ 2025-04-25 - 100 items\n",
      "✅ 2025-04-26 - 100 items\n",
      "✅ 2025-04-27 - 100 items\n",
      "✅ 2025-04-28 - 100 items\n",
      "✅ 2025-04-29 - 100 items\n",
      "✅ 2025-04-30 - 100 items\n",
      "✅ 2025-05-01 - 100 items\n",
      "✅ 2025-05-02 - 100 items\n",
      "✅ 2025-05-03 - 100 items\n",
      "✅ 2025-05-04 - 100 items\n",
      "✅ 2025-05-05 - 100 items\n",
      "✅ 2025-05-06 - 100 items\n",
      "✅ 2025-05-07 - 99 items\n",
      "✅ 2025-05-08 - 100 items\n",
      "✅ 2025-05-09 - 100 items\n",
      "✅ 2025-05-10 - 0 items\n"
     ]
    }
   ],
   "source": [
    "all_news = get_news_data(datetime(2025, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5565b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ DataFrame으로 정리 후 저장\n",
    "df = pd.DataFrame(all_news)\n",
    "df.sort_values(by=\"date\", inplace=True)\n",
    "df = df[[\"date\", \"title\", \"link\", \"source\"]]\n",
    "\n",
    "# ✅ 중복 제거 (title 기준)\n",
    "df.drop_duplicates(subset=\"title\", inplace=True)\n",
    "\n",
    "# 2차로 앞부분 50글자 기준 중복 제거\n",
    "df = remove_duplicate_titles_by_prefix(df, prefix_length=50)\n",
    "\n",
    "# df.to_csv(\"tesla_news.csv\", index=False)\n",
    "# print(\"✅ 저장 완료: tesla_news.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26064d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        res = requests.get(url, headers=headers, timeout=5)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # 기사 본문 파싱: <p> 태그 모두 연결\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = '\\n'.join(p.get_text() for p in paragraphs)\n",
    "\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return \"[ERROR] \" + str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290d515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5998/5998 [1:32:56<00:00,  1.08it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df[\"content\"] = [\n",
    "    extract_article_text(url) for url in tqdm(df[\"link\"][:10])\n",
    "]\n",
    "df.to_csv(\"tesla_news_with_body_2025.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
